{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8870083,"sourceType":"datasetVersion","datasetId":5338273}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"collapsed_sections":["1eG97MUwNSeA"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["## 1.1 Data Collection\n","*   **Gather Data**: Collect the raw data needed for your problem. This could be from databases, APIs, web scraping, sensors, etc.\n","*   **Understand Data Sources**: Ensure you understand the sources and context of your data."],"metadata":{"id":"1eG97MUwNSeA"}},{"cell_type":"code","source":["import pandas as pd\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import re\n","import string\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","import plotly.figure_factory as ff\n","from textblob import TextBlob\n","import numpy as np\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-25T09:01:02.054310Z","iopub.execute_input":"2024-07-25T09:01:02.055135Z","iopub.status.idle":"2024-07-25T09:01:02.064756Z","shell.execute_reply.started":"2024-07-25T09:01:02.055089Z","shell.execute_reply":"2024-07-25T09:01:02.063016Z"},"trusted":true,"id":"2tEm0NAFM29Z","executionInfo":{"status":"ok","timestamp":1722000423293,"user_tz":-330,"elapsed":384,"user":{"displayName":"Ammar Ameerdeen","userId":"05218100029573743201"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n","# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","\n","import os\n","import sys\n","from tempfile import NamedTemporaryFile\n","from urllib.request import urlopen\n","from urllib.parse import unquote, urlparse\n","from urllib.error import HTTPError\n","from zipfile import ZipFile\n","import tarfile\n","import shutil\n","\n","CHUNK_SIZE = 40960\n","DATA_SOURCE_MAPPING = 'sentiment-analysis-for-mental-health:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5338273%2F8870083%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240725%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240725T040914Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D49aa971dd69912a07820b487aba223d32ff06de52de64e03a6cc197f751f3340bbf0a71d20516fb8bd5f757fede5b87897dcdfceb042ede388bcd1fac5221979d11e38c2e1489526c43dfca9e3b79e2681c5aa89dbc920f2e9088eeadf117656c260d8719baff28aa7a24cc824740a354ee7e9610782a520170b67223885be1e8d63a51e53080ad5a6d4b1fd76707877f6d3d9b17495ede21b446e0b227130265f1269c21458b8b53fdd521c3ae5edb29f4304fa48b49c63c4589f7564389197b0402caf6fc6d6c61d6507be2dfe3d527f66b3ffdec1fb2355e6b16c4397a9fe908c053d1bc79931b8e1a4f7c11590d9a49ce30a85d5b2e2ebc83d312848e6f2'\n","\n","KAGGLE_INPUT_PATH='/kaggle/input'\n","KAGGLE_WORKING_PATH='/kaggle/working'\n","KAGGLE_SYMLINK='kaggle'\n","\n","!umount /kaggle/input/ 2> /dev/null\n","shutil.rmtree('/kaggle/input', ignore_errors=True)\n","os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n","os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n","\n","try:\n","  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","try:\n","  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","\n","for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n","    directory, download_url_encoded = data_source_mapping.split(':')\n","    download_url = unquote(download_url_encoded)\n","    filename = urlparse(download_url).path\n","    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n","    try:\n","        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n","            total_length = fileres.headers['content-length']\n","            print(f'Downloading {directory}, {total_length} bytes compressed')\n","            dl = 0\n","            data = fileres.read(CHUNK_SIZE)\n","            while len(data) > 0:\n","                dl += len(data)\n","                tfile.write(data)\n","                done = int(50 * dl / int(total_length))\n","                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n","                sys.stdout.flush()\n","                data = fileres.read(CHUNK_SIZE)\n","            if filename.endswith('.zip'):\n","              with ZipFile(tfile) as zfile:\n","                zfile.extractall(destination_path)\n","            else:\n","              with tarfile.open(tfile.name) as tarfile:\n","                tarfile.extractall(destination_path)\n","            print(f'\\nDownloaded and uncompressed: {directory}')\n","    except HTTPError as e:\n","        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n","        continue\n","    except OSError as e:\n","        print(f'Failed to load {download_url} to path {destination_path}')\n","        continue\n","\n","print('Data source import complete.')\n","\n","\n","# Load the data\n","path = '/kaggle/input/sentiment-analysis-for-mental-health/Combined Data.csv'\n","df = pd.read_csv(path)"],"metadata":{"execution":{"iopub.status.busy":"2024-07-25T09:01:02.068090Z","iopub.execute_input":"2024-07-25T09:01:02.069532Z","iopub.status.idle":"2024-07-25T09:01:03.012270Z","shell.execute_reply.started":"2024-07-25T09:01:02.069491Z","shell.execute_reply":"2024-07-25T09:01:03.010469Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"gWoauDpuM29f","executionInfo":{"status":"ok","timestamp":1722000477717,"user_tz":-330,"elapsed":3788,"user":{"displayName":"Ammar Ameerdeen","userId":"05218100029573743201"}},"outputId":"f1cc9d9e-bf04-4678-8469-91ec03ad84dd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading sentiment-analysis-for-mental-health, 11587194 bytes compressed\n","[==================================================] 11587194 bytes downloaded\n","Downloaded and uncompressed: sentiment-analysis-for-mental-health\n","Data source import complete.\n"]}]},{"cell_type":"markdown","source":["## 1.2. Data Preprocessing\n","*   **Data Cleaning**: Handle missing values, remove duplicates, and correct inconsistencies.\n","*   **Data Transformation**: Convert data into a suitable format, including normalization, scaling, and encoding categorical variables.\n","*   **Feature Engineering**: Create new features that could help improve the model performance. This might include polynomial features, interactions, or domain-specific features.\n","*   **Data Splitting**: Split the dataset into training, validation, and test sets to evaluate the modelâ€™s performance at different stages."],"metadata":{"id":"R-B2y-h5NfWe"}},{"cell_type":"code","source":["# Display the first few rows of the dataframe\n","print(df.head())"],"metadata":{"execution":{"iopub.status.busy":"2024-07-25T09:01:03.014791Z","iopub.execute_input":"2024-07-25T09:01:03.015289Z","iopub.status.idle":"2024-07-25T09:01:03.033775Z","shell.execute_reply.started":"2024-07-25T09:01:03.015248Z","shell.execute_reply":"2024-07-25T09:01:03.032020Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Oml1ZnrKM29g","executionInfo":{"status":"ok","timestamp":1722000483245,"user_tz":-330,"elapsed":663,"user":{"displayName":"Ammar Ameerdeen","userId":"05218100029573743201"}},"outputId":"bb192565-fa57-48ff-9957-30175aaeedf5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["   Unnamed: 0                                          statement   status\n","0           0                                         oh my gosh  Anxiety\n","1           1  trouble sleeping, confused mind, restless hear...  Anxiety\n","2           2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n","3           3  I've shifted my focus to something else but I'...  Anxiety\n","4           4  I'm restless and restless, it's been a month n...  Anxiety\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Ht_cebN4NoGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Missing Values:\")\n","print(df.isnull().sum())\n","# Handle NaN values in the statement column\n","df['statement'] = df['statement'].fillna('')\n","\n","\n","# Data Preprocessing\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","def preprocess_text(text):\n","    text = text.lower()  # Lowercase text\n","    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in square brackets\n","    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove links\n","    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags\n","    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n","    text = re.sub(r'\\n', '', text)  # Remove newlines\n","    text = re.sub(r'\\w*\\d\\w*', '', text)  # Remove words containing numbers\n","    return text\n","\n","df['cleaned_statement'] = df['statement'].apply(lambda x: preprocess_text(x))\n","\n","# Tokenization and Stopwords Removal\n","stop_words = set(stopwords.words('english'))\n","\n","def remove_stopwords(text):\n","    tokens = word_tokenize(text)\n","    tokens = [word for word in tokens if word not in stop_words]\n","    return ' '.join(tokens)\n","\n","df['cleaned_statement'] = df['cleaned_statement'].apply(lambda x: remove_stopwords(x))\n","\n","# Data Augmentation\n","def augment_text(text):\n","    try:\n","        blob = TextBlob(text)\n","        translated = blob.translate(to='fr').translate(to='en')\n","        return str(translated)\n","    except Exception as e:\n","        return text\n","\n","df['augmented_statement'] = df['statement'].apply(augment_text)\n","augmented_df = df[['statement', 'status']].copy()\n","augmented_df['statement'] = df['augmented_statement']\n","df = pd.concat([df, augmented_df])\n","\n","# Reapply preprocessing on augmented data\n","df['cleaned_statement'] = df['statement'].apply(lambda x: preprocess_text(x))\n","df['cleaned_statement'] = df['cleaned_statement'].apply(lambda x: remove_stopwords(x))\n","\n","# Ensure no NaN values are left\n","df['cleaned_statement'] = df['cleaned_statement'].fillna('')"],"metadata":{"execution":{"iopub.status.busy":"2024-07-25T09:01:03.035405Z","iopub.execute_input":"2024-07-25T09:01:03.035767Z","iopub.status.idle":"2024-07-25T09:01:03.094597Z","shell.execute_reply.started":"2024-07-25T09:01:03.035737Z","shell.execute_reply":"2024-07-25T09:01:03.093268Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"M-2NjHkoM29h","executionInfo":{"status":"error","timestamp":1722002191200,"user_tz":-330,"elapsed":1694120,"user":{"displayName":"Ammar Ameerdeen","userId":"05218100029573743201"}},"outputId":"de9c2f6a-19f4-48fa-e1d7-294e412ff843"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Missing Values:\n","Unnamed: 0      0\n","statement     362\n","status          0\n","dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","\n","KeyboardInterrupt\n","\n"]}]},{"cell_type":"code","source":["# prompt: save DF in the mounted drive.\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","df.to_csv('/content/drive/My Drive/colab/cleaned_data.csv', index=False)\n"],"metadata":{"id":"zSIpyy3uOvlt"},"execution_count":null,"outputs":[]}]}